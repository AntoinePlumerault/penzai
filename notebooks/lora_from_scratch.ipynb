{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfejMHs4lr8V"
   },
   "source": [
    "*Copyright 2024 The Penzai Authors.*\n",
    "\n",
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at*\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "*Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USGIPdLYDzSo"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/penzai/blob/main/notebooks/lora_from_scratch.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google-deepmind/penzai/blob/main/notebooks/lora_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrvmM33zdK-Z"
   },
   "source": [
    "# LoRA From Scratch - Patching Pretrained Models in Penzai\n",
    "\n",
    "Penzai is designed to make it easy to make targeted modifications to neural networks after they have been trained. In this notebook, we'll show how to take Penzai's reference implementation of [Gemma 7B](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf) open-weights transformer model, patch it to support Low-Rank Adaptation (LoRA [Hu et al. 2021](https://arxiv.org/abs/2106.09685)), and train the new parameters on a toy problem with a hand-written loss function.\n",
    "\n",
    "The goal of this notebook is to show how *you* could implement something like LoRA from scratch in less than a hundred lines of code, starting from a Penzai implementation of a model that doesn't support it already, and without having to fork the existing implementation source code or even modify the pretrained model's configuration. We'll define everything we need as we go and make changes to models interactively. In fact, our implementation will end up being completely modular; we'll start by applying LoRA to a small MLP and then immediately be able to transfer our implementation to Gemma 7B.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHr2rnIL8DzM"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkW4lYKAu-oR"
   },
   "source": [
    "Before we can get started in earnest, we need to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozG8ERNavDos"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmxgAcFQmZkB"
   },
   "source": [
    "To run this notebook, you need a Python environment with `penzai` and its dependencies installed.\n",
    "\n",
    "In Colab or Kaggle, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGZH58j8mPkj"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import penzai\n",
    "except ImportError:\n",
    "  !pip install penzai[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iog3oMAMGCMG"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v26wYYx6QSn3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import collections\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import orbax.checkpoint\n",
    "import optax\n",
    "from jax.experimental import mesh_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Mh2mAuiQ4aa"
   },
   "outputs": [],
   "source": [
    "import penzai\n",
    "from penzai.deprecated.v1 import pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCMQGV00mZ1"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQU6QfoZVYOL"
   },
   "outputs": [],
   "source": [
    "from penzai.deprecated.v1.example_models import gemma\n",
    "from penzai.deprecated.v1.example_models import simple_mlp\n",
    "from penzai.deprecated.v1.toolshed import basic_training\n",
    "from penzai.toolshed import token_visualization\n",
    "from penzai.deprecated.v1.toolshed import jit_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGzhV5uWvkvB"
   },
   "source": [
    "### Setting up Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjGkV8F8vmpi"
   },
   "source": [
    "For this tutorial, we'll enable Treescope (Penzai's pretty-printer) as the default IPython pretty-printer. This is recommended when using Penzai in an interactive environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YodWk_jmva_7"
   },
   "outputs": [],
   "source": [
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.ts.register_context_manager_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBk_7yT1EwUi"
   },
   "source": [
    "## Intro to Penzai's declarative combinator design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87L-xgdcE5CV"
   },
   "source": [
    "We'll start by giving a brief introduction to Penzai's design conventions, and how they make it easy to insert adapters into pretrained models. Let's begin by initializing a small MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTDKRO7YdI32"
   },
   "outputs": [],
   "source": [
    "mlp = pz.nn.initialize_parameters(\n",
    "    simple_mlp.MLP.from_config([8, 32, 32, 8]),\n",
    "    jax.random.key(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGhVjjC9osuY"
   },
   "source": [
    "Like most Penzai models and layers, this MLP takes named arrays as input and returns them as output. A named array is just a wrapped JAX array where a subset of its positional axes have been tagged with names. (See the [named axes tutorial](named_axes.ipynb) for more info on how to use Penzai's named axis system.)\n",
    "\n",
    "We can call the MLP directly on an array of inputs to run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zs3C95Onoq4Q"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "mlp(pz.nx.NamedArray.wrap(jnp.arange(8, dtype=jnp.float32)).tag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0UpSqo4Dsbu"
   },
   "source": [
    "Penzai models are written in a *declarative*, *combinator*-based style. This means that the structure of the model directly matches the sequence of high-level operations that the model will run in its forward pass. Composite models, like our MLP, just hold onto their sublayers in a list and run these sublayers in order. Primitive layers, like `Linear`, hold on to their parameters as attributes instead of reading them from an external parameter dictionary.\n",
    "\n",
    "We can see the sublayers by pretty-printing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnW8H7n3EMTi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBO5r9ZJEPkK"
   },
   "source": [
    "By convention, most of the \"complicated\" logic in Penzai model classes happens when we initialize them, using the `.from_config` method we called earlier. Once the model is built, the pretty-printed representation provides a full specification of everything the model does, and the parameters are stored as direct attributes on the layers that need them. A general design principle of Penzai is \"*what you see is what you get*\"; you should be able to learn everything you need to know about a model by printing it out.\n",
    "\n",
    "In fact, every Penzai model class can be rebuilt by executing it's pretty-printed representation. You can click on a pretty-printed output and press `r` to add qualified names to the pretty-printed visualization, and if you remove the arrays first, you can then copy and paste the entire pretty-printed code and execute it to make a copy of the model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmUj9haNnOX6"
   },
   "outputs": [],
   "source": [
    "# We can use `eval_shape` to remove array data and just keep the shapes.\n",
    "# Try pressing `r` and copying the below output:\n",
    "jax.eval_shape(lambda: mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi8ueF4yFZqp"
   },
   "outputs": [],
   "source": [
    "penzai.deprecated.v1.example_models.simple_mlp.MLP( # Sequential\n",
    "  sublayers=[\n",
    "    penzai.deprecated.v1.nn.linear_and_affine.Affine( # Sequential\n",
    "      sublayers=[\n",
    "        penzai.deprecated.v1.nn.linear_and_affine.Linear(weights=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 8, 'features_out': 32}), data_array=jax.ShapeDtypeStruct(shape=(8, 32), dtype=np.dtype('float32'))), name='Affine_0.Linear.weights'), in_axis_names=('features',), out_axis_names=('features_out',)),\n",
    "        penzai.deprecated.v1.nn.linear_and_affine.RenameAxes(old=('features_out',), new=('features',)),\n",
    "        penzai.deprecated.v1.nn.linear_and_affine.AddBias(bias=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 32}), data_array=jax.ShapeDtypeStruct(shape=(32,), dtype=np.dtype('float32'))), name='Affine_0.AddBias.bias'), new_axis_names=()),\n",
    "      ],\n",
    "    ),\n",
    "    penzai.deprecated.v1.nn.basic_ops.Elementwise(fn=jax.nn.relu),\n",
    "    penzai.deprecated.v1.nn.linear_and_affine.Affine( # Sequential\n",
    "      sublayers=[penzai.deprecated.v1.nn.linear_and_affine.Linear(weights=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 32, 'features_out': 32}), data_array=jax.ShapeDtypeStruct(shape=(32, 32), dtype=np.dtype('float32'))), name='Affine_1.Linear.weights'), in_axis_names=('features',), out_axis_names=('features_out',)), penzai.deprecated.v1.nn.linear_and_affine.RenameAxes(old=('features_out',), new=('features',)), penzai.deprecated.v1.nn.linear_and_affine.AddBias(bias=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 32}), data_array=jax.ShapeDtypeStruct(shape=(32,), dtype=np.dtype('float32'))), name='Affine_1.AddBias.bias'), new_axis_names=())],\n",
    "    ),\n",
    "    penzai.deprecated.v1.nn.basic_ops.Elementwise(fn=jax.nn.relu),\n",
    "    penzai.deprecated.v1.nn.linear_and_affine.Affine( # Sequential\n",
    "      sublayers=[penzai.deprecated.v1.nn.linear_and_affine.Linear(weights=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 32, 'features_out': 8}), data_array=jax.ShapeDtypeStruct(shape=(32, 8), dtype=np.dtype('float32'))), name='Affine_2.Linear.weights'), in_axis_names=('features',), out_axis_names=('features_out',)), penzai.deprecated.v1.nn.linear_and_affine.RenameAxes(old=('features_out',), new=('features',)), penzai.deprecated.v1.nn.linear_and_affine.AddBias(bias=penzai.deprecated.v1.nn.parameters.Parameter(value=penzai.core.named_axes.NamedArray(named_axes=collections.OrderedDict({'features': 8}), data_array=jax.ShapeDtypeStruct(shape=(8,), dtype=np.dtype('float32'))), name='Affine_2.AddBias.bias'), new_axis_names=())],\n",
    "    ),\n",
    "  ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbnEE6tEFZ6Q"
   },
   "source": [
    "We won't usually do this in practice, because device arrays can't be copy-pasted this way; the parameters will be replaced with placeholder objects. Instead, Penzai provides a sophisticated *selector* system (`pz.select`) that allow us to make targeted modifications to (copies of) models. The point here is that Penzai model objects aren't \"hiding\" anything; they directly expose the structure of their computation as a data structure that can be manipulated.\n",
    "\n",
    "The specific types of Penzai models and composite layers are provided primarily for ease of manipulation and as a way to identify how each part of your model was built. But you can also \"flatten\" a model into a list of sublayers that run in sequence, discarding this extra information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCto92VEGs2J"
   },
   "outputs": [],
   "source": [
    "pz.nn.inline_groups(mlp, lambda _: True, lambda _: True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXeIPCUwGtHT"
   },
   "source": [
    "And you can freely add new logic as well, even if it wasn't configured in the initial model. For instance, here's how you could insert a new layer that prints out its intermediate activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSG1eTVnGyzp"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass  # <- This tags our class as being a Python dataclass and a JAX pytree node.\n",
    "class DisplayIntermediateValue(pz.Layer):  # <- pz.Layer is the base class of Penzai layers.\n",
    "\n",
    "  def __call__(self, intermediate_value):\n",
    "    # Show the value:\n",
    "    pz.show(\"Showing an intermediate value:\", intermediate_value)\n",
    "    # And return it unchanged.\n",
    "    return intermediate_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGwcDOP9oNLF"
   },
   "outputs": [],
   "source": [
    "patched = (\n",
    "    pz.select(mlp)\n",
    "    .at(lambda model: model.sublayers[2])\n",
    "    .insert_after(DisplayIntermediateValue())\n",
    ")\n",
    "pz.select(patched).at_instances_of(DisplayIntermediateValue).show_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaWhMdCKGWhn"
   },
   "source": [
    "`patched` is a *copy* of our model that includes our new layer, and it will run our new logic when the model is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChIKmDvEpRAN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "patched(pz.nx.NamedArray.wrap(jnp.arange(8, dtype=jnp.float32)).tag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hvd2rjPdGzAN"
   },
   "source": [
    "This ability makes it remarkably easy to implement adapters like LoRA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYltKGTd8AwL"
   },
   "source": [
    "## Building a simple LoRA Layer in Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SdGouoz8DDU"
   },
   "source": [
    "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning strategy that augments each linear operation in the model with a decomposed low-rank adapter. The original weight matrix is frozen, and two smaller learnable parameter matrices are used to perturb its output. These parameters are kept separate from the original matrix, so gradients of these new parameters can be easily updated in a compute- and memory-efficient way.\n",
    "\n",
    "The effective weight matrix can be decomposed like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfrJpIBmAEAF"
   },
   "source": [
    "```\n",
    " ┌────────────────┐       ┌─────┐                      \n",
    " │                │       │     │                      \n",
    " │                │       │  A: │   ┌────────────────┐\n",
    " │    W: d*d      │   +   │ d*r │ * │     B: r*d     │\n",
    " │                │       │     │   └────────────────┘\n",
    " │                │       │     │                      \n",
    " └────────────────┘       └─────┘                      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GRJO0e0CI1T"
   },
   "source": [
    "Here `W` is the original frozen weight matrix, `A` is a randomly-initialized matrix, and `B` is initialized to zero to ensure that the adapted model is equivalent to the original one at initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro1gMEmoCxsl"
   },
   "source": [
    "To enable LoRA, we'll do three things for each linear layer in our model:\n",
    "- Freeze the original weight,\n",
    "- Initialize our low-rank matrices A and B,\n",
    "- And replace the original linear layer with the composition of W, A, and B.\n",
    "\n",
    "Let's try it out with a simple MLP like the one we built in the last section. We'll just randomly initialize one for demonstration purposes; in a real LoRA adaptation setting we would generally load this from a pre-trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbp5urEgYF3M"
   },
   "outputs": [],
   "source": [
    "mlp = pz.nn.initialize_parameters(\n",
    "    simple_mlp.MLP.from_config([2, 32, 32, 2]),\n",
    "    jax.random.key(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAqNF0Kmqw1V"
   },
   "source": [
    "### Step 1: Freeze parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQajx3Dipyap"
   },
   "source": [
    "We'll start by freezing all the parameters. Learnable parameters are identifiable because they are instances of `pz.nn.Parameter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp49AKByG9b2"
   },
   "outputs": [],
   "source": [
    "pz.select(mlp).at_instances_of(pz.nn.Parameter).show_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxoZDCsHoZ6L"
   },
   "outputs": [],
   "source": [
    "pz.select(mlp).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO7WWeimqIo_"
   },
   "source": [
    "We can freeze these parameters by replacing each `Parameter` with an equivalent `FrozenParameter`. This is directly tracked inside the structure of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDMmpCpHqVkS"
   },
   "outputs": [],
   "source": [
    "frozen_mlp = pz.select(mlp).at_instances_of(pz.nn.Parameter).apply(\n",
    "    lambda param: pz.nn.FrozenParameter(param.value, param.name)\n",
    ")\n",
    "frozen_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQmBpMfmqnG0"
   },
   "outputs": [],
   "source": [
    "pz.select(frozen_mlp).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlndCo72qq0c"
   },
   "source": [
    "### Step 2: Replace `Linear` layers with low-rank adapted versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGXp8MbdrFts"
   },
   "source": [
    "Next, we'll replace the Linear layers with implementations of LoRA.\n",
    "\n",
    "In essence, a LoRA block is a sum of two computation paths: one that uses the original linear layer, and one that uses a sequence of two linear operations. This pattern can be directly mapped to one of Penzai's simple built-in combinators, `BranchAndAddTogether`. We can take each linear layer, like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "laM_x44Irpsb"
   },
   "outputs": [],
   "source": [
    "frozen_mlp.sublayers[0].sublayers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIuLfQ81ryVM"
   },
   "source": [
    "And replace it with a block like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfPohoo7r0JD"
   },
   "outputs": [],
   "source": [
    "pz.nn.BranchAndAddTogether([\n",
    "    # The original layer with frozen parameters:\n",
    "    pz.nn.NamedGroup(\"Pretrained\", [\n",
    "        frozen_mlp.sublayers[0].sublayers[0],\n",
    "    ]),\n",
    "    # And a low-rank adapter:\n",
    "    pz.nn.NamedGroup(\"Update\", [\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"LoRA-A\",\n",
    "            pz.nn.Linear.from_config(\n",
    "                input_axes={\"features\": 8},\n",
    "                output_axes={\"lowrank\": 2},\n",
    "            ),\n",
    "        ),\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"LoRA-B\",\n",
    "            pz.nn.Linear.from_config(\n",
    "                input_axes={\"lowrank\": 2},\n",
    "                output_axes={\"features_out\": 8},\n",
    "                initializer=pz.nn.zero_initializer,\n",
    "            ),\n",
    "        ),\n",
    "    ]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtRGWzYEuYl5"
   },
   "source": [
    "Note that the above code is a direct translation of a LoRA block into the structure of our model. The matrices A and B are represented as separate Linear blocks inside the overall combinator, and the order of execution is determined by the positions in the `NamedGroup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n5DDQHUu6fR"
   },
   "source": [
    "To simplify the process of making this transformation at every Linear block, we can encapsulate it into a new Layer subclass. Since the computation can already be written as a combination of existing pieces, the idiomatic Penzai approach is to define our new Layer as a subclass of `pz.nn.Sequential`, so that it can be easily flattened (like we did with the MLP) id needed. `Sequential` already defines the necessary attributes and `__call__` method, so we just need to provide a named initializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmOUZn5GvLeH"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass(has_implicitly_inherited_fields=True)\n",
    "class LowRankAdapter(pz.nn.Sequential):\n",
    "\n",
    "  @classmethod\n",
    "  def from_linear(\n",
    "      cls,\n",
    "      linear: pz.nn.Linear,\n",
    "      rank: int,\n",
    "      name: str,\n",
    "      lowrank_axis: str = \"lowrank\",\n",
    "  ) -> 'LowRankAdapter':\n",
    "    \"\"\"Builds a LoRA layer from a Linear layer.\n",
    "\n",
    "    Args:\n",
    "      linear: The linear layer to adapt.\n",
    "      rank: The rank of the low-rank adapter.\n",
    "      name: Prefix for this block's parameters.\n",
    "      lowrank_axis: The axis name for low-rank adaptation.\n",
    "\n",
    "    Returns:\n",
    "      A LoRA block with uninitialized parameters and the same initial\n",
    "      behavior as `linear`.\n",
    "    \"\"\"\n",
    "    return cls([\n",
    "        pz.nn.BranchAndAddTogether([\n",
    "            pz.nn.NamedGroup(\"Pretrained\", [linear]),\n",
    "            pz.nn.NamedGroup(\"Update\", [\n",
    "                pz.nn.add_parameter_prefix(\n",
    "                    name + \"/LoRA_A\",\n",
    "                    pz.nn.Linear.from_config(\n",
    "                        input_axes=linear.input_axes,\n",
    "                        output_axes={lowrank_axis: rank},\n",
    "                        parallel_axes=linear.parallel_axes,\n",
    "                    ),\n",
    "                ),\n",
    "                pz.nn.add_parameter_prefix(\n",
    "                    name + \"/LoRA_B\",\n",
    "                    pz.nn.Linear.from_config(\n",
    "                        input_axes={lowrank_axis: rank},\n",
    "                        output_axes=linear.output_axes,\n",
    "                        parallel_axes=linear.parallel_axes,\n",
    "                        initializer=pz.nn.zero_initializer,\n",
    "                    ),\n",
    "                ),\n",
    "            ]),\n",
    "        ])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6qWE_qKHvFH"
   },
   "source": [
    "Note: Idiomatic Penzai layers generally avoid overriding `__init__`, since dataclasses take their attributes as parameters to `__init__` and we want to ensure the output of the pretty-printer directly corresponds to code we could use to rebuild the model even if we've modified its attributes. When we have nontrivial construction logic, we'll usually define it in a class method like `from_linear` or `from_config` instead.\n",
    "\n",
    "Also, in most Penzai layers, each layer is only responsible for ensuring it's parameter names are *locally* unique, and parent layers add parameter prefixes using `pz.nn.add_parameter_prefix` at each level. In this case, however, we're planning on inserting the LoRA blocks into an existing model, so the names must be *globally* unique. This is why `from_linear` takes a name as an argument but `Linear.from_config` does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id0dsEtUu12s"
   },
   "source": [
    "\n",
    "\n",
    "The next step is to write a helper function for inserting LoRA blocks into a model. We'll use Penzai's `pretty_keystr` function (a fancier version of `jax.tree_util.keystr`) to ensure each block has a unique name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oiUuqWB69KPV"
   },
   "outputs": [],
   "source": [
    "def loraify_all_linears(model, rank: int):\n",
    "  return (\n",
    "      pz.select(model)\n",
    "      .at_instances_of(pz.nn.Linear)\n",
    "      .apply(\n",
    "          lambda keypath, lin: LowRankAdapter.from_linear(\n",
    "              lin,\n",
    "              rank=rank,\n",
    "              name=pz.pretty_keystr(keypath, model),\n",
    "          ),\n",
    "          with_keypath=True,\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSxTRFCFBiDb"
   },
   "source": [
    "Now we can run it on our MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4JiyS9PBlc2"
   },
   "outputs": [],
   "source": [
    "loraified_mlp_uninit = loraify_all_linears(frozen_mlp, rank=2)\n",
    "loraified_mlp_uninit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVMaJL1GB-mA"
   },
   "source": [
    "You can directly check that this transformation is doing the right thing by expanding each `Affine` layer and making sure the `LowRankAdapter` looks right.\n",
    "\n",
    "Note that `loraified_mlp_uninit` is a *copy* of `frozen_mlp` with the requested modifications. In Penzai, transformations of models always return new copies of the model, so you don't have to worry about accidentally making an irreversible change.\n",
    "\n",
    "(Only the model *structure* is copied; the JAX arrays still share memory between the models. But JAX arrays are immutable, so you don't have to worry about those changing either;  unless you explicitly delete or donate them, training loops always return new copies of your model with updated parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vcp8DZHjCxhL"
   },
   "source": [
    "### Step 3: Initializing and training the LoRA weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8TBfRU9C45u"
   },
   "source": [
    "Finally, we can initialize and train the new weights we inserted into the model. To initialize them, we can use the standard Penzai parameter initialization helper function, which finds all `UninitializedParameter` instances and initializes them. In this case, the `UninitializedParameter`s are the LoRA weights, and the `FrozenParameter`s from the pretrained model are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unOomI6EDGze"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "loraified_mlp = pz.nn.initialize_parameters(loraified_mlp_uninit, jax.random.key(42))\n",
    "loraified_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSBVbWHEDWXz"
   },
   "source": [
    "Since we froze the \"pretrained\" parameters before we applied `loraify_all_linears`, the learnable parameters of our new model are just the new LoRA weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaZZd3CRDgLO"
   },
   "outputs": [],
   "source": [
    "pz.select(loraified_mlp).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r27WANoeEa3m"
   },
   "source": [
    "This means you can easily train them using Penzai's basic training loop helpers, or write your own custom training loop for them. As a demonstration, we'll train this model to implement XOR by only fitting the low-rank adapter parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EePTWC2ZEmFK"
   },
   "outputs": [],
   "source": [
    "xor_inputs = pz.nx.wrap(\n",
    "    jnp.array([[-1, -1], [-1, 1], [1, -1], [1, 1]], dtype=jnp.float32),\n",
    "    \"batch\",\n",
    "    \"features\",\n",
    ")\n",
    "xor_labels = jnp.array([[0, 1], [1, 0], [1, 0], [0, 1]], dtype=jnp.float32)\n",
    "\n",
    "def loss_fn(model, rng, state):\n",
    "  assert state is None\n",
    "  model_out = model(xor_inputs)\n",
    "  log_probs = jax.nn.log_softmax(\n",
    "      model_out.unwrap(\"batch\", \"features\"), axis=-1\n",
    "  )\n",
    "  losses = -log_probs * xor_labels\n",
    "  loss = jnp.sum(losses) / 4\n",
    "  return loss, None, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JHLUzcpXKBF"
   },
   "outputs": [],
   "source": [
    "train_step = basic_training.build_train_step_fn(loss_fn)\n",
    "train_state = basic_training.TrainState.initial_state(\n",
    "    model=loraified_mlp,\n",
    "    optimizer_def=optax.adam(0.1),\n",
    "    root_rng=jax.random.key(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wA2aCwHSXQtW"
   },
   "outputs": [],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkO7q6XzXNCG"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  train_state, out = train_step(train_state)\n",
    "  print(i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRrlJCG0YRBj"
   },
   "outputs": [],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOosZhMWJnJF"
   },
   "source": [
    "`TrainState` is an optional utility that manages the optimizer states for us. It also partitions the model into learnable and nonlearnable parts, but we can combine them again by reading the computed property `train_state.model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzjoscuYY047"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "train_state.model(xor_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j8eKyG6YoHH"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "pz.nx.nmap(jnp.argmax)(train_state.model(xor_inputs).untag(\"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZqHvuvuYkkb"
   },
   "source": [
    "Looks like it worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3B0Y9P5oaNc"
   },
   "source": [
    "## Adding LoRA to Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt54N33AZOh5"
   },
   "source": [
    "Let's now try adding LoRA to the Gemma 7B pretrained model. Because of Penzai's compositional design, the implementation in the previous section will just work out of the box!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ISnc54lwJIV"
   },
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJbVbYBewKnm"
   },
   "source": [
    "We'll start by loading the weights from the Gemma checkpoint. We'll use the 7B checkpoint for this tutorial, and shard it over our local devices using JAX's automatic partitioning. (You can read more about JAX's automatic distributed arrays [on this JAX documentation page](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html).)\n",
    "\n",
    "If you prefer, you can also run this tutorial with the 2B checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X3qrwbK4SvX"
   },
   "source": [
    "You can download the Gemma checkpoints using a Kaggle account and an API key. If you don't have an API key already, you can:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account if needed.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Next, if you are running this notebook in Google Colab:\n",
    "\n",
    "1. Click the \"key\" symbol on the left toolbar to open the \"Secrets\" tab.\n",
    "2. Add two new secrets, named \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\", and set their values based on the API key you downloaded.\n",
    "3. Run the cell below and grant this notebook access to the secrets you just made.\n",
    "\n",
    "If you are not running this notebook in Google Colab, you can instead run the cell below, input your username and API key in the textboxes, and click the login button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNgVlaJl2IbZ"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  kagglehub.config.set_kaggle_credentials(\n",
    "      userdata.get(\"KAGGLE_USERNAME\"), userdata.get(\"KAGGLE_KEY\")\n",
    "  )\n",
    "except ImportError:\n",
    "  kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Oxko4R4yaK"
   },
   "source": [
    "If everything went well, you should see:\n",
    "\n",
    "```\n",
    "Kaggle credentials set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUnej-yy5beB"
   },
   "source": [
    "Before downloading Gemma, you will also need to consent to the Gemma Terms of Use. If you haven't done that yet, you can do so here:\n",
    "\n",
    "> https://www.kaggle.com/models/google/gemma/license/consent\n",
    "\n",
    "(Make sure you choose to \"Verify via Kaggle Account\" with the same account you used to log in above!)\n",
    "\n",
    "Once you've agreed to the terms, you can run the next cell to download the Gemma weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmUAGwXE41la"
   },
   "outputs": [],
   "source": [
    "weights_dir = kagglehub.model_download('google/gemma/Flax/7b')\n",
    "ckpt_path = os.path.join(weights_dir, '7b')\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nlbX3C-1sB6"
   },
   "source": [
    "We can then load the SentencePiece vocabulary and restore the checkpointed parameters into JAX using `orbax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEXnGmeUGxCK"
   },
   "outputs": [],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bn6Xwlk3xlr5"
   },
   "outputs": [],
   "source": [
    "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "metadata = checkpointer.metadata(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VewGmKxMwIbs"
   },
   "outputs": [],
   "source": [
    "n_devices = jax.local_device_count()\n",
    "sharding_devices = mesh_utils.create_device_mesh((n_devices,))\n",
    "sharding = jax.sharding.PositionalSharding(sharding_devices)\n",
    "restore_args = jax.tree_util.tree_map(\n",
    "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
    "        restore_type=jax.Array,\n",
    "        sharding=sharding.reshape((1,) * (len(m.shape) - 1) + (n_devices,))\n",
    "    ),\n",
    "    metadata,\n",
    ")\n",
    "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9amupev6WKj"
   },
   "outputs": [],
   "source": [
    "gemma_model = gemma.model_core.GemmaTransformer.from_pretrained(\n",
    "    flat_params,\n",
    "    upcast_activations_to_float32=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxt4YxCZu1fP"
   },
   "outputs": [],
   "source": [
    "del flat_params\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsInFjJ-Znb-"
   },
   "source": [
    "Here's what the Gemma model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGIHE981ZorN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "gemma_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U786gQ5TZC2x"
   },
   "source": [
    "Try clicking the triangle markers to explore the structure of Gemma and look at some of the parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh2OGC5QZdcW"
   },
   "source": [
    "### Converting Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YifL5OPuZfgM"
   },
   "source": [
    "Now we can freeze its parameters and LoRA-ify its linear blocks in the same way that we did for the simple MLP.\n",
    "\n",
    "The Penzai implementation of Gemma uses the same `Linear` layer to implement all of the learnable operations, in both the MLP blocks and the attention blocks. So we'll use a slightly-modified helper function that lets us be more specific about which `Linear` layers we want to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5DTU_XAbepz"
   },
   "outputs": [],
   "source": [
    "def loraify_linears_in_selection(selection, rank: int):\n",
    "  model = selection.deselect()\n",
    "  return (\n",
    "      selection\n",
    "      .at_instances_of(pz.nn.Linear)\n",
    "      .apply(\n",
    "          lambda keypath, lin: LowRankAdapter.from_linear(\n",
    "              lin,\n",
    "              rank=rank,\n",
    "              name=pz.pretty_keystr(keypath, model),\n",
    "          ),\n",
    "          with_keypath=True,\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYHT7vR1bpkP"
   },
   "source": [
    "Now we go through and apply each of the transformation steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7RtmqjbZcy8"
   },
   "outputs": [],
   "source": [
    "# Step 1: Freeze the pretrained parameters.\n",
    "frozen_gemma_model = (\n",
    "    pz.select(gemma_model)\n",
    "    .at_instances_of(pz.nn.Parameter)\n",
    "    .apply(\n",
    "        lambda param: pz.nn.FrozenParameter(param.value, param.name)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p11gtxQUZ31a"
   },
   "outputs": [],
   "source": [
    "# Step 2: LoRA-ify the Linear blocks. Following Hu et al. (2021), we'll only\n",
    "# LoRA-ify the attention parameters.\n",
    "loraified_gemma_model_uninit = loraify_linears_in_selection(\n",
    "    pz.select(frozen_gemma_model).at_instances_of(gemma.model_core.GemmaAttention),\n",
    "    rank=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebe1mz8SaiMk"
   },
   "outputs": [],
   "source": [
    "# Step 3: Initialize the new LoRA parameters.\n",
    "loraified_gemma_model = pz.nn.initialize_parameters(\n",
    "    loraified_gemma_model_uninit, jax.random.key(123)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkhQVq7sa8XD"
   },
   "outputs": [],
   "source": [
    "# Step 4 (optional): Look at it to make sure the transformation looks right.\n",
    "pz.select(loraified_gemma_model).at_instances_of(LowRankAdapter).show_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgtlh83Tcb3W"
   },
   "source": [
    "If we wanted, we could have just as easily adapted the MLP layers, by changing\n",
    "```\n",
    ".at_instances_of(gemma.model_core.GemmaAttention)\n",
    "```\n",
    "to\n",
    "```\n",
    ".at_instances_of(gemma.model_core.GemmaFeedForward)\n",
    "```\n",
    "We could have also customized which blocks have LoRA parameters by using Penzai's selector system (see the separate [selectors tutorial](selectors.ipynb) for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz8hTWZQdZji"
   },
   "source": [
    "### Fine-tuning Gemma with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fciyNWtkdcVg"
   },
   "source": [
    "We can now fine-tune our LoRA-ified Gemma model, with full control over the training loop.\n",
    "\n",
    "For this tutorial, we'll just generate some synthetic data. Specifically, we'll show it some examples of evaluating a mysterious function, and train it to figure out what the function does. We won't worry too much about efficiency of the data pipeline, since our goal is just to show how LoRA fine-tuning could work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-qyTC9QaNx-"
   },
   "outputs": [],
   "source": [
    "def mystery_function(a, b):\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgoxBZN0e-6G"
   },
   "outputs": [],
   "source": [
    "def generate_example(np_rng):\n",
    "  a, b = np_rng.choice(1000, size=(2,))\n",
    "  c = mystery_function(a, b)\n",
    "  return f\">>> mystery_function({a}, {b})\\n{c}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Rjvtb08fW46"
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(examples, pad_length=32, include_eos=True):\n",
    "  padded_tokens = []\n",
    "  for example in examples:\n",
    "    example_tokens = [vocab.bos_id()] + vocab.EncodeAsIds(example)\n",
    "    if include_eos:\n",
    "      example_tokens = example_tokens + [vocab.eos_id()]\n",
    "    assert len(example_tokens) <= pad_length\n",
    "    example_tokens = example_tokens + [vocab.pad_id()] * (pad_length - len(example_tokens))\n",
    "    padded_tokens.append(example_tokens)\n",
    "  return pz.nx.wrap(jnp.array(padded_tokens)).tag(\"batch\", \"seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_-80NmZgsyR"
   },
   "source": [
    "Penzai has some useful utilities for visualizing token arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owQzY2q-gDKi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "np_rng = np.random.default_rng(123)\n",
    "input_examples = tokenize_batch([generate_example(np_rng) for _ in range(20)])\n",
    "input_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTWG27KmgWTX"
   },
   "outputs": [],
   "source": [
    "token_visualization.show_token_array(input_examples, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Zy1I7KWjTNL"
   },
   "source": [
    "Let's train our new parameters on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VScBxB7-jebR"
   },
   "outputs": [],
   "source": [
    "def xent_loss_fn(model, rng, state, input_examples):\n",
    "  del rng, state  # Unused.\n",
    "  # Run the model on shifted examples.\n",
    "  # `GemmaInputs.from_basic_segments` is responsible for building the causal\n",
    "  # attention mask and setting up positional embeddings.\n",
    "  outputs = model(gemma.model_core.GemmaInputs.from_basic_segments(\n",
    "      input_examples[{\"seq\": pz.slice[:-1]}]\n",
    "  ))\n",
    "  # Compute log-probabilities along the \"vocabulary\" axis.\n",
    "  all_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "      outputs.untag(\"vocabulary\")\n",
    "  ).tag(\"vocabulary\")\n",
    "  # Index by the correct tokens.\n",
    "  correct_next_tokens = input_examples[{\"seq\": pz.slice[1:]}]\n",
    "  correct_log_probs = all_log_probs[{\"vocabulary\": correct_next_tokens}]\n",
    "  # Mask padding tokens.\n",
    "  correct_log_probs = pz.nx.nmap(jnp.where)(\n",
    "      correct_next_tokens == vocab.pad_id(),\n",
    "      0.0,\n",
    "      correct_log_probs,\n",
    "  )\n",
    "  # Take averages.\n",
    "  loss = -correct_log_probs.untag(\"batch\", \"seq\").unwrap().mean()\n",
    "  return loss, None, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLiMR9imjXwh"
   },
   "outputs": [],
   "source": [
    "train_step = basic_training.build_train_step_fn(xent_loss_fn, donate_params_and_state=True)\n",
    "train_state = basic_training.TrainState.initial_state(\n",
    "    model=loraified_gemma_model,\n",
    "    optimizer_def=optax.adamw(5e-5, weight_decay=0.01),\n",
    "    root_rng=jax.random.key(42),\n",
    ")\n",
    "np_rng = np.random.default_rng(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwYyQxUun196"
   },
   "outputs": [],
   "source": [
    "# Train on 200 batches of 16 examples -> 3,200 examples\n",
    "# (For reference, there are 1000 * 1000 = 1,000,000 possible examples in the\n",
    "# synthetic distribution we are using.)\n",
    "print_steps = {*range(10), *range(10, 200, 10)}\n",
    "while train_state.step < 200:\n",
    "  input_examples = tokenize_batch([\n",
    "      generate_example(np_rng) for _ in range(16)\n",
    "  ])\n",
    "  train_state, out = train_step(train_state, input_examples=input_examples)\n",
    "  if train_state.step in print_steps:\n",
    "    print(train_state.step, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxdrFk633e3J"
   },
   "source": [
    "To see what the model learned, we can pull out the model from the train state and look at its parameters. In this case, all of the learnable parameters were added by our LoRA adapter.\n",
    "\n",
    "We'll turn on the autovisualizer so that we can see the distribution of values in the arrays at a glance; try clicking on a few to expand their visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Hd8chYFAJH4"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "pz.select(train_state.model).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOVwcpOWBW9Z"
   },
   "source": [
    "Recall that we initialized all of the \"B\" matrices to zero. So the fact that they are no longer zero indicates that the model has definitely learned something!\n",
    "\n",
    "But has it learned what we wanted? Let's try running it on a randomly sampled batch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_hGorZjAIHA"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "np_rng = np.random.default_rng(98765)\n",
    "validation_examples = tokenize_batch([generate_example(np_rng) for _ in range(32)])\n",
    "validation_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i8TUnkWDDu9"
   },
   "outputs": [],
   "source": [
    "token_visualization.show_token_array(validation_examples, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6d-z4mnCNuT"
   },
   "outputs": [],
   "source": [
    "outputs = train_state.model(gemma.model_core.GemmaInputs.from_basic_segments(\n",
    "    validation_examples[{\"seq\": pz.slice[:-1]}]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy35m0sLCoJt"
   },
   "outputs": [],
   "source": [
    "# Compute log-probabilities along the \"vocabulary\" axis.\n",
    "all_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "    outputs.untag(\"vocabulary\")\n",
    ").tag(\"vocabulary\")\n",
    "\n",
    "# Index by the correct tokens.\n",
    "correct_next_tokens = validation_examples[{\"seq\": pz.slice[1:]}]\n",
    "correct_log_probs = all_log_probs[{\"vocabulary\": correct_next_tokens}]\n",
    "\n",
    "# Plot the probability of the correct digit.\n",
    "# This uses the same renderer as %%autovisualize, but doesn't truncate the array\n",
    "# and lets us mask out elements.\n",
    "pz.ts.render_array(\n",
    "    pz.nx.nmap(jnp.exp)(correct_log_probs),\n",
    "    valid_mask=(correct_next_tokens != vocab.pad_id()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo6BmNQxFZn1"
   },
   "source": [
    "We can see that the model is predicting the arguments to `mystery_function` with about 10% accuracy, which is reasonable because those digits are random. It also seems to be almost perfectly accurate on the answers, indicating that it has successfully fit the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogdBDPpPd92G"
   },
   "source": [
    "## Running inference on our LoRA-ified model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pHt23-PBGWI"
   },
   "source": [
    "Now that we've fine-tuned the model, we can convert it into decoding mode and sample from it.\n",
    "\n",
    "In Penzai, autoregressive decoding is performed by a separate class `GemmaKVCachingTransformer`, instead of being an alternative mode of `GemmaTransformer`. This is an instance of a more general pattern in Penzai models: each model and layer does a single thing at runtime, instead of doing different things depending on what arguments you pass. In fact, idiomatic Penzai layers always define a single function `__call__`, and that function always takes a single argument (although that argument can be a dictionary or tuple if needed). This makes it easy to compose many layers together in a uniform way without having to worry about how to handle function arguments.\n",
    "\n",
    "The decoding mode transformation is actually very similar to the LoRA adaptation transformation we defined above. Instead of replacing `Linear` blocks with new `LowRankAdapter` blocks (which have new parameters), this transformation replaces `Attention` blocks with `KVCachingAttention` blocks (which have new state variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEXkCzVCf8HO"
   },
   "source": [
    "Since the key-value caching for Gemma is itself implemented as a patching transformation, this means that key-value caching can be immediately applied to our final `train_state.model` even though we've already edited the model structure to add new adapted parameters. Our modifications don't conflict with the attention block structure, so the modifications can be easily composed.\n",
    "\n",
    "Here's how we can enable decoding mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfX60kwYhVU9"
   },
   "outputs": [],
   "source": [
    "finetuned_inference_model, initial_inference_state = (\n",
    "  gemma.sampling_mode.GemmaKVCachingTransformer.from_uncached(\n",
    "      train_state.model,\n",
    "      cache_len=64,\n",
    "      batch_axes={\"batch\": 4},\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kirZHkTXhqQk"
   },
   "source": [
    "Let's look inside to see the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFO003K_hr_3"
   },
   "outputs": [],
   "source": [
    "# You can use a function to pick out an initial node to expand in the\n",
    "# visualization. (You can also copy such a function by clicking the grey copy\n",
    "# icon at the end of each line.)\n",
    "pz.select(finetuned_inference_model).at(\n",
    "    (lambda root: root.body.body.body.body.body.sublayers[5].sublayers[0].delta.sublayers[1].kv_cache)\n",
    ").show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_TYfG_VhxGW"
   },
   "source": [
    "The `LowRankAdapter` classes we inserted are still there in the model, but there have been a few other changes to the model structure:\n",
    "- The outermost class is of a different type `GemmaKVCachingTransformer`.\n",
    "- Inside it, there's a new `WithFunctionalLocalState` wrapper, which is responsible for managing the key-value caches, and a new `WithSideInputsFromInputTuple` wrapper that manages the current decoding position.\n",
    "- Inside each of the transformer blocks, the `GemmaAttention` layers have been replaced with new `GemmaKVCachingAttention` layers that point back to these two new wrappers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4nRm6JyiY-t"
   },
   "source": [
    "Now that we've converted the model, we can use some existing helper functions to sample from it. (We discuss the decoding mode and helper functions more in the separate [\"Gemma From Scratch\" tutorial](gemma_from_scratch.ipynb). We'll wrap our model in `Jitted` so that it JIT-compiles itself whenever it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-7AvEYO_owI"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \">>> mystery_function(123, 123)\",\n",
    "    \">>> mystery_function(101, 15)\",\n",
    "    \">>> mystery_function(999, 876)\",\n",
    "    \">>>\", # Let the model write and solve its own problem\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apdnwcKv_r1b"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "tokenized_prompts = tokenize_batch(prompts, 16, include_eos=False)\n",
    "tokenized_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeGSSlT63NR5"
   },
   "outputs": [],
   "source": [
    "samples = gemma.simple_decoding_loop.temperature_sample_pyloop(\n",
    "    jit_wrapper.Jitted(finetuned_inference_model),\n",
    "    initial_inference_state,\n",
    "    prompt=tokenized_prompts,\n",
    "    rng=jax.random.key(3),\n",
    "    pad_id=vocab.pad_id(),\n",
    "    max_sampling_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIxoMT_-ASIi"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "pz.show(samples)\n",
    "token_visualization.show_token_array(samples, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj6L5Cr0BhAC"
   },
   "source": [
    "As desired, our fine-tuned model seems to have learned the behavior of `mystery_function` using the low-rank updates to its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJJy4aY-C1Oo"
   },
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfUSaDeKDOV-"
   },
   "source": [
    "This notebook demonstrates how Penzai makes it easy to edit the structure of a pretrained model without requiring any changes to the original model implementation. Our `LowRankAdapter` class and associated utilities took less than a hundred lines of code, and were immediately compatible with the pretrained Gemma 7B model, including both training and sampling modes.\n",
    "\n",
    "The definitions in this notebook are also available in `penzai.toolshed.lora`, and can be imported from there if you are interested in using Penzai to perform parameter-efficient fine-tuning.\n",
    "\n",
    "However, LoRA is just one example of what you can do with Penzai's powerful patching and model rewriting utilities. The key-value caching transformation is another, which we discuss in the [\"Gemma From Scratch\" tutorial](gemma_from_scratch.ipynb). And these tools can also be used to study intermediate activations and perform targeted counterfactual interventions to specific layers in the model, which we discuss in the [\"Induction Heads\" tutorial](induction_heads.ipynb). Penzai is designed to simplify the general process of editing, visualizing, and analyzing pretrained models; the goal is not to implement every possible type of fine-tuning or patching, but instead to give you powerful general-purpose tools and then get out of your way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmZK9WYDLdiD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LoRA From Scratch - Patching Pretrained Models in Penzai",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
